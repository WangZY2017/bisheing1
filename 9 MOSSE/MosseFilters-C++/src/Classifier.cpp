// Classifier.cpp
// This file contains the definition of class Classifier. The steps we follow for 
// classification are as follows,
// 1. Load models generated by the learner
// 2. Create location objects to acquire LOIs
// 3. Extract features using the feature extractor objects
// 4. Normalize using extremal values seen during learning
// 5. Classify

#include "Classifier.h"

// Construction and destruction

// The field outputDirectory will be used to generate all classification data.

Classifier::Classifier(string output, Trainer::KernelType kernel, 
		       Location* le, Location* re, Location* n,
		       roiFnT roiFn) {
  // check if the output directory exists, or else bail
  DIR* dir;
  dir = opendir(output.c_str());
  if (dir == NULL) {
    string err = "Trainer::Trainer. The directory " + output + " does not exist. Bailing out.";
    throw (err);
  }
  closedir(dir);

  outputDirectory = output;
  roiFunction = roiFn;
  kernelType = kernel;

  // Build a set of feature extraction objects and stuff them into the
  // extractor vector
  Feature* f = (Feature*) new FeatureLX();
  featureExtractors.push_back(f);
  f = (Feature*) new FeatureRX();
  featureExtractors.push_back(f);
  f = (Feature*) new FeatureNX();
  featureExtractors.push_back(f);
  f = (Feature*) new FeatureLRDist();
  featureExtractors.push_back(f);
  f = (Feature*) new FeatureLNDist();
  featureExtractors.push_back(f);
  f = (Feature*) new FeatureRNDist();
  featureExtractors.push_back(f);
  f = (Feature*) new FeatureLNAngle();
  featureExtractors.push_back(f);
  f = (Feature*) new FeatureRNAngle();
  featureExtractors.push_back(f);
  f = (Feature*) new FeatureLRNArea();
  featureExtractors.push_back(f);

  nFeatures = (int)Feature::End - 1;

  // read extremal feature values and other data generated during
  // training that we need for classification
  readParameters();

  // create location extractors
  leftEye = le;
  rightEye = re;
  nose = n;

  // load models
  for (unsigned int i = 0; i < Globals::numZones; i++) {
    char buffer[Globals::smallBufferSize];
    sprintf(buffer, "%d.model", i + 1);
    string str = buffer;
    string modelFileName = outputDirectory + "/" + Globals::modelNamePrefix + str;

    MODEL* model = read_model((char*)modelFileName.c_str());
    if (!model) {
      string err = "Classifier::Classifier. read_model returned NULL.";
      throw (err);
    }
    if (model->kernel_parm.kernel_type == 0) { /* linear kernel */
      /* compute weight vector */
      add_weight_vector_to_linear_model(model);
    }
    models.push_back(model);
  }
}

Classifier::~Classifier() {
  for (int i = 0; i < nFeatures; i++)
    delete featureExtractors[i];

  for (unsigned int i = 0; i < Globals::numZones; i++)
    free_model(models[i], 1);
}

// readParameters
// Method that reads parameters.xml from the output directory to get the min
// and max values for each feature that were seen during training.
// We use these values for normalization during classification.
// Note: This means that the training data should cover extremal values
// for each feature

void Classifier::readParameters() {
  string filename = outputDirectory + "/" + Globals::paramsFileName;
  ifstream file;

  file.open((const char*)filename.c_str());
  if (file.good()) {
    string line;
    // skip first two lines
    getline(file, line); getline(file, line);

    int index = 0;
    double min = 0;
    double max = 0;

    while (!file.eof()) {
      getline(file, line);
      if (line.find("<min") != string::npos) {
	const char* token = strtok((char*)line.c_str(), " <>");
	if (token) {
	  token = strtok(NULL, " <>");
	  char* ptr;
	  min = (token)? strtod(token, &ptr) : min;
	}
      } else if (line.find("<max") != string::npos) {
	const char* token = strtok((char*)line.c_str(), " <>");
	if (token) {
	  token = strtok(NULL, " <>");
	  char* ptr;
	  max = (token)? strtod(token, &ptr) : max;
	  featureExtractors[index]->setMinVal(min);
	  featureExtractors[index]->setMaxVal(max);
	  index++;
	}
      }
    }    
    file.close();
  } else {
    string err = "Classifier::readParameters. Cannot open " +
      Globals::paramsFileName + " in output directory " +
      outputDirectory + ".";
    throw (err);
  }
}

// normalize
// Method used to normalize data from a given image using extremal values
// seen during training

void Classifier::normalize(vector<double>& data) {
  // first compute averages and ranges
  vector<double> average;
  vector<double> spread;

  // we normalize all data points to lie in the interval [-1, 1] using the
  // average of the extremal values and their spread
  for (unsigned int i = 0; i < featureExtractors.size(); i++) {
    // feature specific normalization
    double min = featureExtractors[i]->getMinVal();
    double max = featureExtractors[i]->getMaxVal();

    average.push_back((max + min) / 2.0);
    spread.push_back(max - min);
  }

  // normalize values
  #pragma omp parallel for num_threads(omp_get_max_threads())
  for (unsigned int i = 0; i < data.size(); i++) {
    data[i] = (data[i] - average[i]) / spread[i]; 
  }
}

// getZone
// Method used to get the zone corresponding to the user gaze in a given image
// frame. The method uses the location extractors to extract LOIs and then 
// compute features using the feature extractors. It then normalizes them and
// get the gaze zone with the highest likelihood.
// The offset input parameter is used to compute the true locations of interest
// when filters are created for user defined regions of interest

// NOTE: The method expects that the frame annotation object has been annotated
// with an approximate face center. If we find that that is not the case then
// we pick the image center. During classification, to carve out the roi we 
// need a reference, namely center of the face when the driver is looking 
// straight ahead. We use this to determine the locations of the eyes and using
// those we determine the location of the nose. For each step a window function
// is used that is centered at the center of the ROI

int Classifier::getZone(IplImage* frame, double& confidence, FrameAnnotation& fa) {
  if (!leftEye || !rightEye || !nose) {
    string err = "Classifier::getZone. Location extractors malformed.";
    throw (err);
  }

  // the roi offset
  CvPoint offset;

  // LOIs
  CvPoint leftEyeLocation;
  CvPoint rightEyeLocation;
  CvPoint noseLocation;

  // computing the confidence of the location identification
  double leftPSR;
  double rightPSR;
  double nosePSR;

  CvPoint center = fa.getLOI(Annotations::Face);
  if (!center.x || !center.y) {
    center.x = Globals::imgWidth / 2;
    center.y = Globals::imgHeight / 2;
    fa.setFace(center);
  }

  offset.x = offset.y = 0;
  IplImage* roi = (roiFunction)? roiFunction(frame, fa, offset, Annotations::Face) : 0;

  // all location extractors do identical preprocessing. Therefore, preprocess
  // once using say the left eye extractor and re-use it for all three extractors
  fftw_complex* preprocessedImage = leftEye->getPreprocessedImage((roi)? roi : frame);

  #pragma omp parallel sections num_threads(2)
  {
    #pragma omp section
    {
      leftEye->setImage(preprocessedImage);
      leftEye->apply();
      leftEye->getMaxLocation(leftEyeLocation, leftPSR);
      leftEyeLocation.x += offset.x;
      leftEyeLocation.y += offset.y;
    }

    #pragma omp section
    {
      // get the location of the right eye
      rightEye->setImage(preprocessedImage);
      rightEye->apply();
      rightEye->getMaxLocation(rightEyeLocation, rightPSR);
      rightEyeLocation.x += offset.x;
      rightEyeLocation.y += offset.y;
    }
  }

  if (roi)
    cvReleaseImage(&roi);

  center.x = (leftEyeLocation.x + rightEyeLocation.x) / 2;
  center.y = leftEyeLocation.y + Globals::noseDrop;

  fa.setNose(center);

  offset.x = offset.y = 0;
  roi = (roiFunction)? roiFunction(frame, fa, offset, Annotations::Nose) : 0;

  // free the preprocessed image
  fftw_free(preprocessedImage);

  // all location extractors do identical preprocessing. Therefore, preprocess
  // once using say the left eye extractor and re-use it for all three extractors
  preprocessedImage = nose->getPreprocessedImage((roi)? roi : frame);

  // get the location of the nose
  nose->setImage(preprocessedImage);
  nose->apply();
  nose->getMaxLocation(noseLocation, nosePSR);
  noseLocation.x += offset.x;
  noseLocation.y += offset.y;

  // free the preprocessed image
  fftw_free(preprocessedImage);

  fa.setLeftIris(leftEyeLocation);
  fa.setRightIris(rightEyeLocation);
  fa.setNose(noseLocation);

  // we are done with the images at this point. Free roi if not zero
  if (roi)
    cvReleaseImage(&roi);

  //  cout << "Confidence (L, R, N) = (" << leftPSR << ", " <<
  //    rightPSR << ")" << endl;

  // extract features vector
  vector<double> data;
  for (int i = 0; i < nFeatures; i++) {
    double value = featureExtractors[i]->extract(&fa);
    data.push_back(value);
  }

  // normalize
  normalize(data);

  // create SVM Light objects to classify
  DOC* doc;
  WORD* words = (WORD*)malloc(sizeof(WORD) * (nFeatures + 1));

  for (int i = 0; i < nFeatures; i++) {
    words[i].wnum = featureExtractors[i]->getId();
    words[i].weight = data[i];
  }

  // SVM Light expects that the features vector has a zero element
  // to indicate termination and hence
  words[nFeatures].wnum = 0;
  words[nFeatures].weight = 0.0;

  // create doc
  string comment = "Gaze SVM";
  doc = create_example(-1, 0, 0, 0.0, create_svector(words, (char*)comment.c_str(), 1.0));

  int maxIndex = 0;
  confidence = -FLT_MAX;

  double dists[Globals::numZones];

  // classify using each zone model
  #pragma omp parallel for num_threads(Globals::numZones)
  for (unsigned int i = 0; i < Globals::numZones; i++) {
    if (kernelType == Trainer::Linear)
      dists[i] = classify_example_linear(models[i], doc);
    else
      dists[i] = classify_example(models[i], doc);
  }
  
  for (unsigned int i = 0; i < Globals::numZones; i++) {
    if (confidence < dists[i]) {
      confidence = dists[i];
      maxIndex = i + 1;
    }
  }

  free_example(doc, 1);
  free(words);

  return maxIndex;
}

// getFilterError
// Method that computes the error in locating LOIs for this filter with respect 
// to all the annotated frames in a given training set. The operation is 
// cummulative

double Classifier::getFilterError(string trainingDirectory, Annotations::Tag tag,
				  ErrorType errorType) {
  Annotations annotations;
  Filter* filter = getFilter(tag);

  // read annotations
  string locationsFileName = trainingDirectory + "/" + Globals::annotationsFileName;
  annotations.readAnnotations(locationsFileName);

  // get the frames directory
  string framesDirectory = annotations.getFramesDirectory();

  // reset total
  double totalError = 0;

  // iterate over the set of all annotations
  vector<FrameAnnotation*>& frameAnnotations = annotations.getFrameAnnotations();
  for (unsigned int i = 0; i < frameAnnotations.size(); i++) {
    FrameAnnotation* fa = frameAnnotations[i];

    // get LOI
    CvPoint& location = fa->getLOI(tag);
    if (!location.x && !location.y)
      continue;

    // compose filename
    char buffer[256];
    sprintf(buffer, "frame_%d.png", fa->getFrameNumber());
    string fileName = framesDirectory + "/" + buffer;

    // load image
    IplImage* inputImg = cvLoadImage((const char*)fileName.c_str());
    if (!inputImg) {
      string err = "Filter::update. Cannot load file " + fileName + ".";
      throw (err);
    }
    IplImage* image = cvCreateImage(cvGetSize(inputImg), IPL_DEPTH_8U, 1);
    cvCvtColor(inputImg, image, CV_BGR2GRAY);

    // get the location of the left eye
    CvPoint offset;
    offset.x = offset.y = 0;
    IplImage* roi = (roiFunction)? roiFunction(image, *fa, offset, Annotations::Face) : 0;

    location.x -= offset.x;
    location.y -= offset.y;

    // apply filter
    fftw_complex* imageFFT = filter->preprocessImage((roi)? roi : image);
    IplImage* postFilterImg = filter->apply(imageFFT);
    
    // compute location
    double min;
    double max;
    CvPoint minLoc;
    CvPoint maxLoc;
    cvMinMaxLoc(postFilterImg, &min, &max, &minLoc, &maxLoc);

    // compute squared error as the distance between the location
    // found and the location as annotated
    double xdiff = abs(maxLoc.x - location.x);
    double ydiff = abs(maxLoc.y - location.y);

    switch (errorType) {
    case OneNorm:
      totalError += (xdiff + ydiff);
      break;
    case TwoNorm:
      totalError += sqrt(xdiff * xdiff + ydiff * ydiff);
      break;
    case MSE:
      totalError += (xdiff * xdiff + ydiff * ydiff);
      break;
    default:
      totalError += ((xdiff > ydiff)? xdiff : ydiff);
      break;
    }

    if (roi)
      cvReleaseImage(&roi);
    cvReleaseImage(&image);
    cvReleaseImage(&inputImg);
  }

  return totalError / frameAnnotations.size();
}

// getError
// Method used to compute an error percentage using the ratio of the number of
// zones misclassified versus the number of zones correctly classified

pair<double,string> Classifier::getError(string trainingDirectory) {
  Annotations annotations;

  // read annotations
  string locationsFileName = trainingDirectory + "/" + Globals::annotationsFileName;
  annotations.readAnnotations(locationsFileName);

  // get the frames directory
  string framesDirectory = annotations.getFramesDirectory();

  double missclassified = 0;

  int counts[Globals::numZones];
  int missCounts[Globals::numZones];
  for (unsigned int i = 0; i < 3; i++) {
    counts[i] = 0;
    missCounts[i] = 0;
  }

  // iterate over the set of all annotations
  vector<FrameAnnotation*>& frameAnnotations = annotations.getFrameAnnotations();
  for (unsigned int i = 0; i < frameAnnotations.size(); i++) {
    FrameAnnotation* fa = frameAnnotations[i];

    int actualZone = fa->getZone();
    if (actualZone < 3)
	actualZone = 1;
    else if (actualZone > 3)
	actualZone = 3;
    else
	actualZone = 2;
    counts[actualZone - 1]++;

    // compose filename
    char buffer[256];
    sprintf(buffer, "frame_%d.png", fa->getFrameNumber());
    string fileName = framesDirectory + "/" + buffer;

    // load image
    IplImage* inputImg = cvLoadImage((const char*)fileName.c_str());

    double confidence;
    FrameAnnotation tf;
    int zone = getZone(inputImg, confidence, tf);
    if (zone < 3)
	zone = 1;
    else if (zone > 3)
	zone = 3;
    else
	zone = 2;

    if (zone != actualZone) {
      cout << "Classifier::getError. Expecting zone " << actualZone << 
	" got zone " << zone << endl;
      missclassified++;
      missCounts[actualZone - 1]++;
    }

    cvReleaseImage(&inputImg);
  }

  int nAnnotations = frameAnnotations.size();
  char buffer[Globals::largeBufferSize];
  sprintf(buffer, "%d out of %d were miss-classified.", (int)missclassified, nAnnotations);
  string msg = buffer;
  sprintf(buffer, " Zones [%d, %d, %d].", counts[0], counts[1], counts[2]);
  msg += buffer;
  sprintf(buffer, " Missed [%d, %d, %d].", missCounts[0], missCounts[1], missCounts[2]);
  msg += buffer;

  return make_pair((missclassified / frameAnnotations.size()) * 100, msg);
}
